{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Metrics-only pipeline (requests + responses + Excel overview), no predictions\n",
    "# 1) Group by (RouteId, Date) from ModifiedQueryRows.xlsx\n",
    "# 2) Identify: first morning CreateSequence, final pre-departure EstimateTime\n",
    "# 3) Parse both request files to get coords & time windows; parse both response files to get sequences\n",
    "# 4) Compute metrics: removed/added, order-change (footrule, Kendall tau), position-shift stats,\n",
    "#    relative path lengths and % change, optional time-window stratifications.\n",
    "#\n",
    "# NOTE: For now distances are in normalized units. % change is still meaningful.\n",
    "\n",
    "import re, json, math, pandas as pd, numpy as np\n",
    "\n",
    "def parse_request_coords(path):\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "            tasks = data['tasks']\n",
    "            return {int(t['id']):(float(t['address']['latitude']), float(t['address']['longitude']))\n",
    "                    for t in tasks}\n",
    "        except json.JSONDecodeError:\n",
    "            text = f.read()\n",
    "            return {int(m.group(1)):(float(m.group(2)), float(m.group(3)))\n",
    "                    for m in re.finditer(r'id\\\\s+(\\\\d+)\\\\s+address\\\\s+latitude\\\\s+([\\\\d\\\\.\\\\-]+)\\\\s+longitude\\\\s+([\\\\d\\\\.\\\\-]+)', text)}\n",
    "\n",
    "def parse_response_ids(path):\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        return [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "\n",
    "def path_length(seq, coords):\n",
    "    total = 0.0\n",
    "    for i in range(len(seq)-1):\n",
    "        a, b = coords.get(seq[i]), coords.get(seq[i+1])\n",
    "        if a and b:\n",
    "            dx, dy = a[0]-b[0], a[1]-b[1]\n",
    "            total += math.hypot(dx, dy)\n",
    "    return total\n",
    "\n",
    "def metrics_for_pair(req_first_json, resp_first_txt, req_final_json, resp_final_txt):\n",
    "    coords_first = parse_request_coords(req_first_json)\n",
    "    coords_final = parse_request_coords(req_final_json)\n",
    "    seq_first = parse_response_ids(resp_first_txt)\n",
    "    seq_final = parse_response_ids(resp_final_txt)\n",
    "\n",
    "    set_first, set_final = set(seq_first), set(seq_final)\n",
    "    removed = sorted(set_first - set_final)\n",
    "    added   = sorted(set_final - set_first)\n",
    "    common  = sorted(set_first & set_final)\n",
    "\n",
    "    pos_first = {tid:i for i,tid in enumerate(seq_first)}\n",
    "    pos_final = {tid:i for i,tid in enumerate(seq_final)}\n",
    "    positions = np.array([[tid, pos_first[tid], pos_final[tid]] for tid in common])\n",
    "    abs_shifts = np.abs(positions[:,1] - positions[:,2])\n",
    "\n",
    "    # Footrule\n",
    "    footrule = float(abs_shifts.sum())\n",
    "    n = len(common); max_footrule = n*(n-1)/2 if n>1 else 1\n",
    "    footrule_norm = footrule/max_footrule\n",
    "\n",
    "    # Kendall tau\n",
    "    perm = [pos_final[tid] for tid in sorted(common, key=lambda x: pos_first[x])]\n",
    "    inv = sum(perm[i] > perm[j] for i in range(len(perm)) for j in range(i+1, len(perm)))\n",
    "    max_inv = n*(n-1)//2 if n>1 else 1\n",
    "    kendall_norm = inv/max_inv\n",
    "\n",
    "    # Path lengths\n",
    "    L_first   = path_length(seq_first,  coords_first)\n",
    "    L_final   = path_length(seq_final,  coords_final)\n",
    "    L_delta   = L_final - L_first\n",
    "    L_pct     = (L_delta/L_first*100) if L_first>0 else float('nan')\n",
    "\n",
    "    return {\n",
    "        'removed_count': len(removed), 'added_count': len(added), 'common_count': n,\n",
    "        'footrule': footrule, 'footrule_norm': footrule_norm,\n",
    "        'kendall_inversions': int(inv), 'kendall_tau_norm': kendall_norm,\n",
    "        'mean_abs_shift': float(abs_shifts.mean()) if n else 0.0,\n",
    "        'median_abs_shift': float(np.median(abs_shifts)) if n else 0.0,\n",
    "        'max_abs_shift': int(abs_shifts.max()) if n else 0,\n",
    "        'length_first_units': L_first, 'length_final_units': L_final,\n",
    "        'length_delta_units': L_delta,        'length_delta_units': L_delta, 'length_pct_change': L_pct,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Build stable location_id\n",
    "#    - either hash(round(lat, 6), round(lon, 6))\n",
    "#    - parse request -> {taskId: (lat, lon), time_window}#    - or use a small-radius match by KDTree/DBSCAN if jitter exists\n",
    "#    - parse response -> sequence_first, sequence_final\n",
    "#    - map each taskId -> location_id\n",
    "#    - compute removed/added/common (on taskId AND on location_id for robustness)\n",
    "#    - compute order-change metrics on the common set\n",
    "#    - compute path length (units) + % change\n",
    "#    - compute centroid and bbox of the day's stops\n",
    "\n",
    "# 3) Across days per RouteId:\n",
    "#    - group by RouteId; compute centroid drift, bbox drift, Jaccard overlap of location_id sets\n",
    "#    - aggregate per-location stats: days_seen, removal_rate, avg_abs_shift, etc.\n",
    "``\n",
    "\n",
    "# 2) For each (RouteId, Date):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # file: build_route_metrics.py\n",
    "# # Scans data/requests and data/responses for depot 0521# Python 3.9+  (uses: pandas, numpy, pathlib)\n",
    "# # Pairs first/final (and optional intermediates) per (RouteId, Date)\n",
    "# # Computes metrics and writes consolidated CSVs\n",
    "# # ------------------------------------------------------------\n",
    "\n",
    "# import re, json, math, sys\n",
    "# from pathlib import Path\n",
    "# from datetime import datetime, time\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # =========================\n",
    "# # Configuration\n",
    "# # =========================\n",
    "# BASE_DIR = Path(\"data\")                # adjust if your repo layout differs\n",
    "# REQUESTS_DIR = BASE_DIR / \"requests\"\n",
    "# RESPONSES_DIR = BASE_DIR / \"responses\"\n",
    "# EXCEL_OVERVIEW = BASE_DIR / \"ModifiedQueryRows.xlsx\"  # optional but recommended\n",
    "\n",
    "# DEPOT_PREFIX = \"0521_\"                 # only process route folders whose name starts with this\n",
    "# MORNING_START = time(5, 0, 0)          # 05:00 local\n",
    "# MORNING_END   = time(13, 0, 0)         # 13:00 local (exclude evening investigations)\n",
    "# INCLUDE_EVOLUTION = True               # set False if you only want first vs final\n",
    "\n",
    "# # Output files\n",
    "# OUT_DIR = Path(\"out\")\n",
    "# OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# MASTER_METRICS_CSV = OUT_DIR / \"master_route_day_metrics.csv\"\n",
    "# PER_CALL_METRICS_CSV = OUT_DIR / \"per_call_metrics.csv\"\n",
    "# PER_TASK_LABELS_CSV = OUT_DIR / \"per_route_day_task_labels.csv\"  # optional (large)\n",
    "# CALL_INDEX_CSV = OUT_DIR / \"call_index.csv\"                      # traceability\n",
    "\n",
    "\n",
    "# # =========================\n",
    "# # Helpers: parsing & time\n",
    "# # =========================\n",
    "# FILENAME_TS_RE = re.compile(r\".*-(\\d{6,6})-\")  # e.g. 0521_300-20220617-055733-2-0.json -> 055733\n",
    "\n",
    "# def parse_hhmmss_from_fname(name: str) -> time:\n",
    "#     m = FILENAME_TS_RE.match(name)\n",
    "#     if not m:\n",
    "#         # fallback: try last 6 digits pattern\n",
    "#         m = re.search(r\"(\\d{6})(?=[^0-9]*\\.)\", name)\n",
    "#     if not m:\n",
    "#         return None\n",
    "#     hh, mm, ss = m.group(1)[0:2], m.group(1)[2:4], m.group(1)[4:6]\n",
    "#     return time(int(hh), int(mm), int(ss))\n",
    "\n",
    "# def is_morning(t: time) -> bool:\n",
    "#     return (t is not None) and (MORNING_START <= t < MORNING_END)\n",
    "\n",
    "# def try_read_json(path: Path):\n",
    "#     try:\n",
    "#         return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "#     except json.JSONDecodeError:\n",
    "#         return None\n",
    "\n",
    "# TASK_BLOCK_RE = re.compile(\n",
    "#     r\"id\\s+(\\d+)\\s+address\\s+latitude\\s+([-\\d\\.]+)\\s+longitude\\s+([-\\d\\.]+)\\s+timeWindow\\s+from\\s+([0-9\\-:T]+)\\s+till\\s+([0-9\\-:T]+)\",\n",
    "#     re.MULTILINE\n",
    "# )\n",
    "# FIXED_BLOCK_RE = re.compile(\n",
    "#     r\"taskId\\s+(\\d+)\\s+activityType\\s+Task\\s+fixedPosition\\s+(true|false)\",\n",
    "#     re.MULTILINE\n",
    "# )\n",
    "\n",
    "# def parse_request(path: Path):\n",
    "#     \"\"\"\n",
    "#     Returns dict with:\n",
    "#       configurationName: str|None\n",
    "#       tasks: list of {id:int, lat:float, lon:float, from:str, till:str}\n",
    "#       fixedTasks: set[int]\n",
    "#     \"\"\"\n",
    "#     data = try_read_json(path)\n",
    "#     out = {\"configurationName\": None, \"tasks\": [], \"fixedTasks\": set()}\n",
    "#     if data:\n",
    "#         out[\"configurationName\"] = data.get(\"configurationName\")\n",
    "#         for t in data.get(\"tasks\", []):\n",
    "#             addr = t.get(\"address\", {})\n",
    "#             out[\"tasks\"].append({\n",
    "#                 \"id\": int(t[\"id\"]),\n",
    "#                 \"lat\": float(addr.get(\"latitude\", np.nan)),\n",
    "#                 \"lon\": float(addr.get(\"longitude\", np.nan)),\n",
    "#                 \"from\": t.get(\"timeWindow\", {}).get(\"from\"),\n",
    "#                 \"till\": t.get(\"timeWindow\", {}).get(\"till\"),\n",
    "#             })\n",
    "#         out[\"fixedTasks\"] = {int(ft.get(\"taskId\")) for ft in data.get(\"fixedTasks\", []) if \"taskId\" in ft}\n",
    "#         return out\n",
    "\n",
    "#     # Fallback for flattened text\n",
    "#     txt = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "#     # config name\n",
    "#     m = re.search(r\"configurationName\\s+([A-Za-z]+)\", txt)\n",
    "#     if m: out[\"configurationName\"] = m.group(1)\n",
    "#     # tasks\n",
    "#     for m in TASK_BLOCK_RE.finditer(txt):\n",
    "#         out[\"tasks\"].append({\n",
    "#             \"id\": int(m.group(1)),\n",
    "#             \"lat\": float(m.group(2)),\n",
    "#             \"lon\": float(m.group(3)),\n",
    "#             \"from\": m.group(4),\n",
    "#             \"till\": m.group(5),\n",
    "#         })\n",
    "#     # fixed\n",
    "#     for m in FIXED_BLOCK_RE.finditer(txt):\n",
    "#         out[\"fixedTasks\"].add(int(m.group(1)))\n",
    "#     return out\n",
    "\n",
    "# def parse_response_ids(path: Path) -> list[int]:\n",
    "#     if not path.exists():\n",
    "#         return []\n",
    "#     ids = []\n",
    "#     for line in path.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines():\n",
    "#         s = line.strip()\n",
    "#         if s.isdigit():\n",
    "#             ids.append(int(s))\n",
    "#     return ids\n",
    "\n",
    "\n",
    "# # =========================\n",
    "# # Index the calls\n",
    "# # =========================\n",
    "# def index_calls():\n",
    "#     \"\"\"\n",
    "#     Returns DataFrame with columns:\n",
    "#       depot, route_id, date, call_time, folder_type (request|response),\n",
    "#       config_name (for requests), num_tasks (for requests), num_fixed (for requests),\n",
    "#       request_path, response_path, call_key\n",
    "#     We build one row per REQUEST call and attach the nearest-in-time RESPONSE path in same folder.\n",
    "#     \"\"\"\n",
    "#     rows = []\n",
    "\n",
    "#     # (A) Optional: read Excel overview to help with typing\n",
    "#     overview = None\n",
    "#     if EXCEL_OVERVIEW.exists():\n",
    "#         try:\n",
    "#             overview = pd.read_excel(EXCEL_OVERVIEW, engine=\"openpyxl\")\n",
    "#             # Normalize column names\n",
    "#             overview.columns = [c.strip() for c in overview.columns]\n",
    "#             # Make a key RouteId + Date + Time to help later if needed\n",
    "#         except Exception as e:\n",
    "#             print(f\"[WARN] Could not read {EXCEL_OVERVIEW}: {e}\")\n",
    "\n",
    "#     # Walk request folders\n",
    "#     for route_day_dir in sorted(REQUESTS_DIR.glob(f\"{DEPOT_PREFIX}*\")):\n",
    "#         if not route_day_dir.is_dir():\n",
    "#             continue\n",
    "#         # route_day name pattern: 0521_300-20220617\n",
    "#         folder_name = route_day_dir.name\n",
    "#         if \"-\" not in folder_name:\n",
    "#             continue\n",
    "#         route_id, ymd = folder_name.split(\"-\")\n",
    "#         if not route_id.startswith(DEPOT_PREFIX):\n",
    "#             continue\n",
    "#         date_str = ymd  # yyyymmdd\n",
    "\n",
    "#         resp_dir = RESPONSES_DIR / folder_name\n",
    "\n",
    "#         # requests in this folder\n",
    "#         for req_file in sorted(route_day_dir.glob(\"*.json\")):\n",
    "#             call_t = parse_hhmmss_from_fname(req_file.name)\n",
    "#             req_parsed = parse_request(req_file)\n",
    "#             config = req_parsed.get(\"configurationName\")\n",
    "#             num_tasks = len(req_parsed.get(\"tasks\", []))\n",
    "#             num_fixed = len(req_parsed.get(\"fixedTasks\", []))\n",
    "\n",
    "#             # find a response file with same timestamp; fallback: nearest in time\n",
    "#             candidate = None\n",
    "#             if resp_dir.exists():\n",
    "#                 # first try exact hhmmss match\n",
    "#                 exact = list(resp_dir.glob(req_file.name.replace(\".json\", \".txt\")))\n",
    "#                 if exact:\n",
    "#                     candidate = exact[0]\n",
    "#                 else:\n",
    "#                     # fallback: nearest by hhmmss among files that start with same prefix (route_id-date-*)\n",
    "#                     resp_files = sorted(resp_dir.glob(f\"{route_id}-{date_str}-*.txt\"))\n",
    "#                     # compute abs diff in seconds\n",
    "#                     def hhmmss_to_sec(t: time):\n",
    "#                         return t.hour*3600 + t.minute*60 + t.second if t else None\n",
    "#                     req_sec = hhmmss_to_sec(call_t)\n",
    "#                     best = None\n",
    "#                     best_delta = None\n",
    "#                     for rf in resp_files:\n",
    "#                         rt = parse_hhmmss_from_fname(rf.name)\n",
    "#                         rs = hhmmss_to_sec(rt)\n",
    "#                         if (req_sec is not None) and (rs is not None):\n",
    "#                             delta = abs(req_sec - rs)\n",
    "#                             if best is None or delta < best_delta:\n",
    "#                                 best = rf\n",
    "#                                 best_delta = delta\n",
    "#                     candidate = best\n",
    "\n",
    "#             rows.append({\n",
    "#                 \"depot\": route_id.split(\"_\")[0],\n",
    "#                 \"route_id\": route_id,\n",
    "#                 \"date\": date_str,\n",
    "#                 \"call_time\": call_t.isoformat() if call_t else None,\n",
    "#                 \"config_name\": config,\n",
    "#                 \"num_tasks\": num_tasks,\n",
    "#                 \"num_fixed\": num_fixed,\n",
    "#                 \"request_path\": str(req_file),\n",
    "#                 \"response_path\": str(candidate) if candidate else None,\n",
    "#             })\n",
    "\n",
    "#     df = pd.DataFrame(rows)\n",
    "#     # Morning filter flag\n",
    "#     def flag_morning(t):\n",
    "#         if pd.isna(t): return False\n",
    "#         tt = datetime.strptime(t, \"%H:%M:%S\").time()\n",
    "#         return is_morning(tt)\n",
    "#     df[\"is_morning\"] = df[\"call_time\"].apply(flag_morning)\n",
    "\n",
    "#     # Persist for transparency\n",
    "#     df.sort_values([\"route_id\", \"date\", \"call_time\"], inplace=True, na_position=\"last\")\n",
    "#     df.to_csv(CALL_INDEX_CSV, index=False)\n",
    "#     return df\n",
    "\n",
    "\n",
    "# # =========================\n",
    "# # Pair first & final + mark intermediates\n",
    "# # =========================\n",
    "# def select_pairs(call_index: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     For each (route_id, date):\n",
    "#       first = earliest morning CreateSequence\n",
    "#       final = last morning EstimateTime with num_fixed == num_tasks\n",
    "#       intermediates = morning calls in between (optional)\n",
    "#     \"\"\"\n",
    "#     records = []\n",
    "#     for (route_id, date), grp in call_index.groupby([\"route_id\", \"date\"], dropna=False):\n",
    "#         g = grp[grp[\"is_morning\"]].copy()\n",
    "#         if g.empty:\n",
    "#             continue\n",
    "\n",
    "#         # First CreateSequence\n",
    "#         g_first = g[g[\"config_name\"]==\"CreateSequence\"].sort_values(\"call_time\")\n",
    "#         if g_first.empty:\n",
    "#             continue\n",
    "#         first_row = g_first.iloc[0]\n",
    "\n",
    "#         # Final: last EstimateTime with fully fixed plan\n",
    "#         g_final = g[(g[\"config_name\"]==\"EstimateTime\") & (g[\"num_fixed\"]==g[\"num_tasks\"])].sort_values(\"call_time\")\n",
    "#         if g_final.empty:\n",
    "#             # fallback: last morning EstimateTime even if not fully fixed\n",
    "#             g_final = g[g[\"config_name\"]==\"EstimateTime\"].sort_values(\"call_time\")\n",
    "#             if g_final.empty:\n",
    "#                 continue\n",
    "#         final_row = g_final.iloc[-1]\n",
    "\n",
    "#         # Intermediates: all morning calls strictly between first and final times\n",
    "#         inter = pd.DataFrame()\n",
    "#         if INCLUDE_EVOLUTION:\n",
    "#             inter = g[\n",
    "#                 (g[\"call_time\"] > first_row[\"call_time\"]) &\n",
    "#                 (g[\"call_time\"] < final_row[\"call_time\"])\n",
    "#             ].sort_values(\"call_time\")\n",
    "\n",
    "#         records.append({\n",
    "#             \"route_id\": route_id,\n",
    "#             \"date\": date,\n",
    "#             \"first_request_path\": first_row[\"request_path\"],\n",
    "#             \"first_response_path\": first_row[\"response_path\"],\n",
    "#             \"first_call_time\": first_row[\"call_time\"],\n",
    "#             \"final_request_path\": final_row[\"request_path\"],\n",
    "#             \"final_response_path\": final_row[\"response_path\"],\n",
    "#             \"final_call_time\": final_row[\"call_time\"],\n",
    "#             \"num_intermediate_calls\": len(inter),\n",
    "#             \"intermediate_rows\": inter  # keep for now; we won't serialize this dict\n",
    "#         })\n",
    "\n",
    "#     return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "# # =========================\n",
    "# # Metrics & labels\n",
    "# # =========================\n",
    "# def path_length(seq: list[int], coords: dict[int, tuple[float,float]]) -> float:\n",
    "#     total = 0.0\n",
    "#     for i in range(len(seq)-1):\n",
    "#         a = coords.get(seq[i]); b = coords.get(seq[i+1])\n",
    "#         if not a or not b: continue\n",
    "#         dx = a[0]-b[0]; dy = a[1]-b[1]\n",
    "#         total += math.hypot(dx, dy)\n",
    "#     return total\n",
    "\n",
    "# def parse_coords_and_seq(req_path: str, resp_path: str):\n",
    "#     req = parse_request(Path(req_path))\n",
    "#     coords = {t[\"id\"]:(t[\"lat\"], t[\"lon\"]) for t in req[\"tasks\"] if not (np.isnan(t[\"lat\"]) or np.isnan(t[\"lon\"]))}\n",
    "#     seq = parse_response_ids(Path(resp_path)) if resp_path else []\n",
    "#     return req, coords, seq\n",
    "\n",
    "# def order_metrics(first_seq, final_seq):\n",
    "#     set_first = set(first_seq); set_final = set(final_seq)\n",
    "#     removed = sorted(set_first - set_final)\n",
    "#     added   = sorted(set_final - set_first)\n",
    "#     common  = sorted(set_first & set_final)\n",
    "\n",
    "#     pos_first = {tid:i for i,tid in enumerate(first_seq)}\n",
    "#     pos_final = {tid:i for i,tid in enumerate(final_seq)}\n",
    "\n",
    "#     if len(common)==0:\n",
    "#         return {\n",
    "#             \"removed_count\": len(removed), \"added_count\": len(added), \"common_count\": 0,\n",
    "#             \"footrule\": np.nan, \"footrule_norm\": np.nan,\n",
    "#             \"kendall_inversions\": np.nan, \"kendall_tau_norm\": np.nan,\n",
    "#             \"mean_abs_shift\": np.nan, \"median_abs_shift\": np.nan, \"max_abs_shift\": np.nan,\n",
    "#             \"removed_ids\": removed, \"added_ids\": added, \"shifts_df\": pd.DataFrame()\n",
    "#         }\n",
    "\n",
    "#     positions = np.array([[tid, pos_first[tid], pos_final[tid]] for tid in common])\n",
    "#     abs_shifts = np.abs(positions[:,1] - positions[:,2])\n",
    "\n",
    "#     # Spearman footrule\n",
    "#     footrule = float(abs_shifts.sum())\n",
    "#     n = len(common); max_footrule = n*(n-1)/2 if n>1 else 1.0\n",
    "#     footrule_norm = footrule / max_footrule\n",
    "\n",
    "#     # Kendall tau inversions (O(n^2) is fine for ~200)\n",
    "#     perm = [pos_final[tid] for tid in sorted(common, key=lambda x: pos_first[x])]\n",
    "#     inv = 0\n",
    "#     for i in range(len(perm)):\n",
    "#         for j in range(i+1, len(perm)):\n",
    "#             inv += 1 if perm[i] > perm[j] else 0\n",
    "#     max_inv = n*(n-1)//2 if n>1 else 1\n",
    "#     kendall_norm = inv / max_inv\n",
    "\n",
    "#     shifts_df = pd.DataFrame({\n",
    "#         \"taskId\": positions[:,0].astype(int),\n",
    "#         \"pos_first\": positions[:,1].astype(int),\n",
    "#         \"pos_final\": positions[:,2].astype(int),\n",
    "#         \"abs_shift\": abs_shifts.astype(int)\n",
    "#     })\n",
    "\n",
    "#     return {\n",
    "#         \"removed_count\": len(removed), \"added_count\": len(added), \"common_count\": n,\n",
    "#         \"footrule\": footrule, \"footrule_norm\": footrule_norm,\n",
    "#         \"kendall_inversions\": int(inv), \"kendall_tau_norm\": float(kendall_norm),\n",
    "#         \"mean_abs_shift\": float(abs_shifts.mean()), \"median_abs_shift\": float(np.median(abs_shifts)),\n",
    "#         \"max_abs_shift\": int(abs_shifts.max()),\n",
    "#         \"removed_ids\": removed, \"added_ids\": added, \"shifts_df\": shifts_df\n",
    "#     }\n",
    "\n",
    "# def time_window_category(tw_from: str, tw_till: str):\n",
    "#     try:\n",
    "#         till = datetime.fromisoformat(tw_till).time()\n",
    "#     except Exception:\n",
    "#         return \"unknown\"\n",
    "#     if till.hour==23 and till.minute==59:\n",
    "#         return \"all-day\"\n",
    "#     if till < time(13,0,0):\n",
    "#         return \"morning-window\"\n",
    "#     # quick heuristic for late starts\n",
    "#     try:\n",
    "#         frm = datetime.fromisoformat(tw_from).time()\n",
    "#         if frm >= time(16,0,0):\n",
    "#             return \"late-window\"\n",
    "#     except Exception:\n",
    "#         pass\n",
    "#     return \"other-window\"\n",
    "\n",
    "\n",
    "# # =========================\n",
    "# # Main driver\n",
    "# # =========================\n",
    "# def main():\n",
    "#     call_index = index_calls()\n",
    "#     pairs = select_pairs(call_index)\n",
    "\n",
    "#     master_rows = []\n",
    "#     per_call_rows = []\n",
    "#     per_task_rows = []\n",
    "\n",
    "#     for _, row in pairs.iterrows():\n",
    "#         route_id = row[\"route_id\"]; date = row[\"date\"]\n",
    "#         first_req, first_coords, first_seq = parse_coords_and_seq(row[\"first_request_path\"], row[\"first_response_path\"])\n",
    "#         final_req, final_coords, final_seq = parse_coords_and_seq(row[\"final_request_path\"], row[\"final_response_path\"])\n",
    "\n",
    "#         # Core order metrics\n",
    "#         om = order_metrics(first_seq, final_seq)\n",
    "\n",
    "#         # Path lengths (normalized units)\n",
    "#         L_first = path_length(first_seq, first_coords)\n",
    "#         L_final = path_length(final_seq, final_coords)\n",
    "#         L_delta = L_final - L_first\n",
    "#         L_pct   = (L_delta/L_first*100.0) if L_first>0 else np.nan\n",
    "\n",
    "#         master_rows.append({\n",
    "#             \"depot\": route_id.split(\"_\")[0],\n",
    "#             \"route_id\": route_id,\n",
    "#             \"date\": date,\n",
    "#             \"first_call_time\": row[\"first_call_time\"],\n",
    "#             \"final_call_time\": row[\"final_call_time\"],\n",
    "#             \"num_intermediate_calls\": row[\"num_intermediate_calls\"],\n",
    "#             # order change (common)\n",
    "#             \"removed_count\": om[\"removed_count\"],\n",
    "#             \"added_count\": om[\"added_count\"],\n",
    "#             \"common_count\": om[\"common_count\"],\n",
    "#             \"footrule\": om[\"footrule\"],\n",
    "#             \"footrule_norm\": om[\"footrule_norm\"],\n",
    "#             \"kendall_inversions\": om[\"kendall_inversions\"],\n",
    "#             \"kendall_tau_norm\": om[\"kendall_tau_norm\"],\n",
    "#             \"mean_abs_shift\": om[\"mean_abs_shift\"],\n",
    "#             \"median_abs_shift\": om[\"median_abs_shift\"],\n",
    "#             \"max_abs_shift\": om[\"max_abs_shift\"],\n",
    "#             # path length\n",
    "#             \"length_first_units\": L_first,\n",
    "#             \"length_final_units\": L_final,\n",
    "#             \"length_delta_units\": L_delta,\n",
    "#             \"length_pct_change\": L_pct,\n",
    "#         })\n",
    "\n",
    "#         # Optional per-task labels for this route-day\n",
    "#         set_first = set(first_seq); set_final = set(final_seq)\n",
    "#         removed_ids = set_first - set_final\n",
    "#         added_ids   = set_final - set_first\n",
    "#         kept_ids    = set_first & set_final\n",
    "\n",
    "#         # build time window lookup from first/final requests\n",
    "#         tw = {}\n",
    "#         for t in first_req[\"tasks\"]:\n",
    "#             tw[t[\"id\"]] = (t[\"from\"], t[\"till\"])\n",
    "#         for t in final_req[\"tasks\"]:\n",
    "#             if t[\"id\"] not in tw:\n",
    "#                 tw[t[\"id\"]] = (t[\"from\"], t[\"till\"])\n",
    "\n",
    "#         # shifts for kept\n",
    "#         shifts_map = {}\n",
    "#         for _, r2 in om[\"shifts_df\"].iterrows():\n",
    "#             shifts_map[int(r2[\"taskId\"])] = int(r2[\"abs_shift\"])\n",
    "\n",
    "#         for tid in removed_ids:\n",
    "#             frm, till = tw.get(tid, (None, None))\n",
    "#             per_task_rows.append({\n",
    "#                 \"route_id\": route_id, \"date\": date, \"taskId\": tid,\n",
    "#                 \"label\": \"removed\", \"abs_shift\": None,\n",
    "#                 \"time_window_cat\": time_window_category(frm, till)\n",
    "#             })\n",
    "#         for tid in added_ids:\n",
    "#             frm, till = tw.get(tid, (None, None))\n",
    "#             per_task_rows.append({\n",
    "#                 \"route_id\": route_id, \"date\": date, \"taskId\": tid,\n",
    "#                 \"label\": \"added\", \"abs_shift\": None,\n",
    "#                 \"time_window_cat\": time_window_category(frm, till)\n",
    "#             })\n",
    "#         for tid in kept_ids:\n",
    "#             frm, till = tw.get(tid, (None, None))\n",
    "#             per_task_rows.append({\n",
    "#                 \"route_id\": route_id, \"date\": date, \"taskId\": tid,\n",
    "#                 \"label\": \"kept\", \"abs_shift\": shifts_map.get(tid, 0),\n",
    "#                 \"time_window_cat\": time_window_category(frm, till)\n",
    "#             })\n",
    "\n",
    "#         # Optional: evolution across intermediate calls\n",
    "#         if INCLUDE_EVOLUTION:\n",
    "#             inter_df: pd.DataFrame = row[\"intermediate_rows\"]\n",
    "#             # Compare each intermediate to the first\n",
    "#             for _, ir in inter_df.iterrows():\n",
    "#                 _, inter_coords, inter_seq = parse_coords_and_seq(ir[\"request_path\"], ir[\"response_path\"])\n",
    "#                 em = order_metrics(first_seq, inter_seq)\n",
    "#                 per_call_rows.append({\n",
    "#                     \"route_id\": route_id,\n",
    "#                     \"date\": date,\n",
    "#                     \"call_time\": ir[\"call_time\"],\n",
    "#                     \"config_name\": ir[\"config_name\"],\n",
    "#                     \"num_tasks\": ir[\"num_tasks\"],\n",
    "#                     \"num_fixed\": ir[\"num_fixed\"],\n",
    "#                     \"vs_first_common_count\": em[\"common_count\"],\n",
    "#                     \"vs_first_footrule_norm\": em[\"footrule_norm\"],\n",
    "#                     \"vs_first_kendall_tau_norm\": em[\"kendall_tau_norm\"],\n",
    "#                 })\n",
    "\n",
    "#     # Write outputs\n",
    "#     pd.DataFrame(master_rows).sort_values([\"route_id\",\"date\"]).to_csv(MASTER_METRICS_CSV, index=False)\n",
    "#     if INCLUDE_EVOLUTION and len(per_call_rows)>0:\n",
    "#         pd.DataFrame(per_call_rows).to_csv(PER_CALL_METRICS_CSV, index=False)\n",
    "#     if len(per_task_rows)>0:\n",
    "#         pd.DataFrame(per_task_rows).to_csv(PER_TASK_LABELS_CSV, index=False)\n",
    "\n",
    "#     print(f\"[OK] Wrote:\\n- {MASTER_METRICS_CSV}\")\n",
    "#     if INCLUDE_EVOLUTION and len(per_call_rows)>0:\n",
    "#         print(f\"- {PER_CALL_METRICS_CSV}\")\n",
    "#     if len(per_task_rows)>0:\n",
    "#         print(f\"- {PER_TASK_LABELS_CSV}\")\n",
    "#     print(f\"- {CALL_INDEX_CSV}  (traceability)\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "   \n",
    "# # ------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "pattern = re.compile(r\"^(?:[^-]*-){2}(\\d{6})-\")\n",
    "m = pattern.match(\"0521_300-20220617-055733-2-0.json\")\n",
    "print(m.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"0521_300-20220617-055733-2-0.json\"\n",
    "pattern = re.compile(r\"^(?:[^-]*-)(\\d{8})-\")\n",
    "timestring_from_filename = pattern.match(name)\n",
    "print(timestring_from_filename.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path(\"data\")   \n",
    "print(base_dir)             # adjust if your repo layout differs\n",
    "# REQUESTS_DIR = BASE_DIR / \"requests\"\n",
    "# RESPONSESDIR = BASE_DIR / \"responses\"\n",
    "# EXCEL_OVERVIEW = BASE_DIR / \"ModifiedQueryRows.xlsx\"  # optional but recommended\n",
    "\n",
    "# DEPOT_PREFIX = \"0521_\"                 # only process route folders whose name starts with this\n",
    "# MORNING_START = time(5, 0, 0)          # 05:00 local\n",
    "# MORNING_END   = time(13, 0, 0)         # 13:00 local (exclude evening investigations)\n",
    "# INCLUDE_EVOLUTION = True               # set False if you only want first vs final_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Directory of the current Python file\n",
    "HERE = Path.cwd()\n",
    "BASE_DIR = HERE.parent          # go up to repo/ from src/my_app/\n",
    "DATA_DIR = BASE_DIR / \"data\"            # repo/data\n",
    "OUTPUT = DATA_DIR / \"output\"          # repo/data/outputs\n",
    "\n",
    "OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(HERE)\n",
    "print(BASE_DIR)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "driver-preferences",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
